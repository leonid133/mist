mist.context-defaults.spark-conf = {
  #spark.master = "mesos://zk://10.21.49.3:5181,10.21.49.10:5181,10.21.49.20:5181,10.21.49.30:5181,10.21.49.40:5181/homer/mesos"
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  spark.ui.port = 4069
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
  #spark.sql.warehouse.dir = "maprfs://mapr5-002/user/hive/warehouse/reports.db"
  #spark.sql.warehouse.dir = "maprfs://mapr4-122/user/hive/warehouse/reports.db"
  #spark.driver.extraClassPath = "/nethome/lblokhin/jars/postgresql-9.4.1212.jar"
  #spark.sql.hive.convertMetastoreParquet = "true"
  #spark.sql.hive.convertMetastoreParquet.mergeSchema = "true"
  #spark.sql.hive.exec.dynamic.partition = "true"
  #spark.sql.hive.exec.dynamic.partition.mode = "nonstrict"
  #spark.sql.parquet.mergeSchema = "true"
}

#mist.hive.test = false

mist.http.on = true
mist.http.host = "10.21.49.162"
mist.http.port = 2003
mist.http.router-config-path = "configs/router_nd.conf"

mist.mqtt.on = true
mist.mqtt.host = "rabbitmq.9dev.io"
mist.mqtt.port = 1883
mist.mqtt.subscribe-topic = "pca_mist_sub"
mist.mqtt.publish-topic = "pca_mist_pub"

mist.recovery.on = false
mist.recovery.multilimit = 1
mist.recovery.typedb = "MapDb"
mist.recovery.dbfilename = "file.db"

mist.workers.runner = "local" # or "docker"
mist.workers.host = "10.21.49.162" # default for unix socker
mist.workers.port = 80

mist.context.context-defaults.timeout = 2 days

mist.context-defaults.disposable = true
mist.context-defaults.worker-downtime = 2 days
#mist.context-defaults.run-options = "--packages com.nd.location:api:1.2.9-SNAPSHOT,mysql:mysql-connector-java:6.0.5,com.jiwire:jiwire-grs:3.1.0-SNAPSHOT --exclude-packages com.jiwire:jicore --jars /nethome/lblokhin/jars/pca-scala-assembly-1.2-SNAPSHOT.jar,/nethome/lblokhin/jars/postgresql-8.4-703.jdbc3.jar"
mist.context-defaults.run-options = "--packages com.nd.location:api:1.2.9-SNAPSHOT,mysql:mysql-connector-java:6.0.5,com.jiwire:jiwire-grs:3.1.0-SNAPSHOT --exclude-packages com.jiwire:jicore --jars /nethome/lblokhin/git/pca-experimental/pca/sbt/pca-scala/target/scala-2.11/pca-scala-assembly-1.2-SNAPSHOT.jar,/nethome/lblokhin/jars/postgresql-8.4-703.jdbc3.jar"

mist.context.PCA_EP-TaskCreateRefTables.spark-conf = {
  spark.ui.port = 4070
}

mist.context.PCA_EP-TaskFilterIDs.spark-conf = {
  spark.ui.port = 4071
}

mist.context.PCA_EP-TaskFilterEventsCA.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4072
}

mist.context.PCA_EP-TaskFilterEvents.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4073
}

mist.context.PCA_EP-TaskWriteNewEvents.spark-conf = {
  spark.dynamicAllocation.enabled  = "false"
  spark.speculation = "false"
  spark.task.cpus = "1"
  spark.ui.port = 4074
}

mist.context.PCA_EP-TaskFilterRefTables.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4075
}

mist.context.PCA_EP-TaskRunPCA.spark-conf = {
  spark.speculation = "true"
  spark.task.cpus = "1"
  spark.network.timeout = 300000
  spark.memory.fraction = 0.15
  spark.memory.storageFraction = 0.4
  spark.ui.port = 4076
}

mist.context.PCA_EP-TaskOutputPCA.spark-conf = {
  spark.ui.port = 4077
}

mist.context.PCA_EP-EventLogTracking.spark-conf = {
  spark.eventLog.enabled = "false"
  spark.ui.port = 4078
}

mist.context.PCA_EP-TestParaccelSource.spark-conf = {
  spark.ui.port = 4079
}

###

mist.context.PCA_EP-Create_Ref_Tables.spark-conf = {
  spark.ui.port = 4070
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"

}

mist.context.PCA_EP-filter_IDA_task.spark-conf = {
  spark.ui.port = 4071
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-filter_CA_events_task.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4072
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-filter_events_task.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4073
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-write_new_events.spark-conf = {
  spark.dynamicAllocation.enabled  = "false"
  spark.speculation = "false"
  spark.task.cpus = "1"
  spark.ui.port = 4074
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"
  
  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"

  
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-filter_ref_tables.spark-conf = {
  spark.speculation = "false"
  spark.ui.port = 4075
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"

  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"

  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-PCA_run_task.spark-conf = {
  spark.speculation = "true"
  spark.task.cpus = "1"
  spark.network.timeout = 300000
  spark.memory.fraction = 0.15
  spark.memory.storageFraction = 0.4
  spark.ui.port = 4076
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"

  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"

  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-output_taskAll.spark-conf = {
  spark.ui.port = 4077
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"

  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-EventLogTracking.spark-conf = {
  spark.eventLog.enabled = "false"
  spark.ui.port = 4078
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"

  spark.ui.showConsoleProgress = "false"

  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}

mist.context.PCA_EP-TestParaccel.spark-conf = {
  spark.ui.port = 4079
  
  spark.cores.max = 1500
  spark.mesos.executor.memoryOverhead = 1000
  spark.executor.memory = "8100M"

  spark.ui.showConsoleProgress = "false"
  spark.eventLog.enabled ="true"
  spark.dynamicAllocation.enabled = "false"
  spark.speculation = "true"
  spark.broadcast.compress = "true"
  spark.rdd.compress = "true"
  spark.speculation.quantile = "0.01"
  spark.speculation.multiplier = "1.1"
  spark.scheduler.mode = "FAIR"
  spark.dynamicAllocation.executorIdleTimeout = "240s"
  spark.dynamicAllocation.schedulerBacklogTimeout = "1s"
  spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = "1s"
  spark.kryoserializer.buffer.max = "250m"
  spark.default.parallelism = 1000
  spark.sql.shuffle.partitions = 1000
  spark.eventLog.dir ="maprfs://mapr5-002/sas/pca/eventlog/"
}
            
            
mist.akka {
  remote {
    log-remote-lifecycle-events = off
    log-recieved-messages = off
    netty.tcp {
      hostname = "10.21.49.162"
    }
    transport-failure-detector {
      heartbeat-interval = 30s
      acceptable-heartbeat-pause = 5s
    }
  }
  # Event handlers to register at boot time (Logging$DefaultLogger logs to STDOUT)
  loggers = ["akka.event.Logging$DefaultLogger"]
  cluster {
    seed-nodes = ["akka.tcp://mist@10.21.49.162:2551"]
    auto-down-unreachable-after = 10s
  }
}

mist.main.akka {
  remote.netty.tcp.port = 2551
}

mist.worker.akka {
  remote.netty.tcp.port = 0
}
